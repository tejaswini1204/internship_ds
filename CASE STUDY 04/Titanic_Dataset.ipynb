{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4GfrYLCuCvuNFYUsmsNrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swarup1606/DATA_SCIENCE_INTERNSHIP/blob/main/Titanic_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Du_19xP0Od",
        "outputId": "b65b79dc-de4e-4fde-a1ad-1beb67da136c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.53125\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.61      0.55        76\n",
            "           1       0.57      0.46      0.51        84\n",
            "\n",
            "    accuracy                           0.53       160\n",
            "   macro avg       0.54      0.53      0.53       160\n",
            "weighted avg       0.54      0.53      0.53       160\n",
            "\n",
            "Confusion Matrix:\n",
            " [[46 30]\n",
            " [45 39]]\n"
          ]
        }
      ],
      "source": [
        "#Logistic Regression\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load data and select relevant columns\n",
        "df = pd.read_csv(\"tested.csv\")\n",
        "df = df[['Pclass', 'Sex', 'Survived', 'Age']]\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "\n",
        "# Convert categorical variables into numeric variables\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Feature selection\n",
        "X = df[['Pclass', 'Sex', 'Age']]\n",
        "y = df['Survived']\n",
        "\n",
        "# Handle outliers using a function\n",
        "def handle_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df[column] = df[column].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
        "\n",
        "handle_outliers(df, 'Age')\n",
        "\n",
        "# Handle data imbalance using SMOTE\n",
        "smote = SMOTE(random_state=0)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "# Dimensionality reduction using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X = pca.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
        "\n",
        "# Initialize and train logistic regression model\n",
        "logr = LogisticRegression()\n",
        "logr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = logr.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
        "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Continue with the same dataframe `df` loaded previously\n",
        "\n",
        "# No need to handle outliers and imbalance again\n",
        "# PCA reduction is also not necessary for Naive Bayes\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
        "\n",
        "# Initialize and train Naive Bayes classifier\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('Naive Bayes Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
        "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APMRVPSbTxMZ",
        "outputId": "3c951c92-2479-4387-c53e-3d4d26e6fcdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.6\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.70      0.62        76\n",
            "           1       0.65      0.51      0.57        84\n",
            "\n",
            "    accuracy                           0.60       160\n",
            "   macro avg       0.61      0.60      0.60       160\n",
            "weighted avg       0.61      0.60      0.60       160\n",
            "\n",
            "Confusion Matrix:\n",
            " [[53 23]\n",
            " [41 43]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Continue with the same dataframe `df` loaded previously\n",
        "\n",
        "# No need to handle outliers and imbalance again\n",
        "# PCA reduction is also not necessary for KNN\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
        "\n",
        "# Initialize and train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('KNN Accuracy:', accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slyWbIeQUXiU",
        "outputId": "a6f35113-4e74-490e-f35e-56ddc7ea3ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.70625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Continue with the same dataframe `df` loaded previously\n",
        "\n",
        "# No need to handle outliers and imbalance again\n",
        "# PCA reduction is also not necessary for Decision Tree\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
        "\n",
        "# Initialize and train Decision Tree classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('Decision Tree Accuracy:', accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_iy-gp2Ug8q",
        "outputId": "c9303902-374f-4044-8547-d8cd24576b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Continue with the same dataframe `df` loaded previously\n",
        "\n",
        "# No need to handle outliers and imbalance again\n",
        "# PCA reduction is also not necessary for Random Forest\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
        "\n",
        "# Initialize and train Random Forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('Random Forest Accuracy:', accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzjTNmVYVaAS",
        "outputId": "014e07a0-7a95-428d-f90d-c2651781e8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.8625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Continue with the same dataframe `df` loaded previously\n",
        "\n",
        "# No need to handle outliers and imbalance again\n",
        "# PCA reduction is also not necessary for Gradient Boosting\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
        "\n",
        "# Initialize and train Gradient Boosting classifier\n",
        "gbm = GradientBoostingClassifier(n_estimators=10)\n",
        "gbm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gbm.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('Gradient Boosting Accuracy:', accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTGrXeJUVkv8",
        "outputId": "d0df2ba9-aa30-457d-ea63-ecd5edbccd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Accuracy: 0.675\n"
          ]
        }
      ]
    }
  ]
}